---
layout: single
excerpt: ""
title: "정보 검색(Information Retrieval) 평가는 어떻게 하는 것이 좋을까?(1/2)"
date: 2020-03-16 23:07:00 +0900
tags: information-retrieval evaluation accuracy precision recall
---

오늘 날 우리는 엄청나게 많은 컨텐츠 속에 살고 있다. 아마 사용자가 이 많은 컨텐츠를 일일히 찾아보고 자신이 원하는 정보를 찾는다는 것은 불가능에 가까울 것이다. 컨텐츠 기반의 모든 서비스는 사용자의 정보를 활용하여 수많은 컨텐츠 중 사용자가 원하는 양질의 정보를 사용자에게 제공하기 위해 노력한다.(수 만개의 뉴스 중 사용자의 입맛에 맞는 뉴스만 선별하여 제공하고, 업로드된 이미지로 사물을 인식하여 사용자에게 관련 정보를 알려주는 것처럼 말이다.)

이렇게 인기 컨텐츠, 키워드 검색과 같이 다양한 형태로 원하는 내용과 관련있는 결과를 얻어내는 것을 정보 검색(Information Retrieval, 이하 IR)이라고 한다. IR의 정확도 및 효율을 분석하기 위해 여러가지 평가 기준이 존재하는데, 이 글에서는 평가모델로 사용할 수 있는 다양한 기준들에 대해 알아보고자 한다.

## Accuracy, Precision, Recall

처음으로 다룰 내용은 정확도(accuracy), 정밀도(precision), 재현율(recall)이다.

이 세 가지 내용을 설명하기 전에 아래 그림을 보면서 필요한 개념에 대해 먼저 이해를 해보도록 하자.

![precision_recall]({{site_url}}/assets/images/2020-03-16-01-01.png)

위 그림에서 네모박스 안에 조그마한 원은 IR의 대상이 되는 각각의 컨텐츠를 의미한다. 그 중, 속이 차있는 원은 관련된 컨텐츠(relavant elements)이고, 속이 비어있는 원은 관련없는 컨텐츠이다. 또한, 큰 원안에 들어있는 조그만 원들은 IR에 의해 선택되어진 컨텐츠(selected elements)이고, 녹색 반원에 포함된 조그만 원들은 선택된 컨텐츠 중 관련된 컨텐츠, 붉은 반원에 포함된 조그만 원들은 선택되었으나 관련이 없는 컨텐츠이다.

이 내용을 더 명시적인 도표로 표현하면 아래와 같다.

![precision_recall2]({{site_url}}/assets/images/2020-03-16-01-02.png)

1. true positives(tp) = 선택된 결과(positives)가 관련이 있어서 선택과 정답이 일치하는(true) 경우
1. true negetives(tn) = 선택되지 않은 결과(negatives)가 관련이 없어서 선택과 정답이 일치하는(true) 경우
1. false positives(fp) = 선택된 결과(positives)가 관련이 없어서 선택과 정답이 일치하지 않는(false) 경우
1. false negatives(fn) = 선택되지 않은 결과(negatives)가 관련이 있어서 선택과 정답이 일치하지 않는(false) 경우

이렇게 결과에 따라 4가지 경우로 분류할 수 있는데, 4가지 경우의 값을 조합하여 아래와 같이 정확도, 정밀도, 재현율을 구할 수 있다.

1. 정확도(accuracy) = tp + tn / tp + tn + fp + fn 
1. 정밀도(precision) = tp / tp + fp
1. 재현율(recall) = tp / tp + fn

정확도는 모든 컨텐츠 중 IR의 결과가 정답과 일치하는 비율을 구한다. 이는 정밀도나 재현율과 다르게 전체 컨텐츠에 대한 IR의 신뢰도를 판단할 수 있는 근거가 된다.
정밀도는 IR이 선택한 결과 중 관련이 있는 비율을 구한다. 이는 가장 간단하고 직접적인 평가 기준으로, IR이 선택한 컨텐츠가 얼마나 정확한가를 판단하는 기준이 된다.
재현율은 관련있는 컨텐츠 중 IR이 선택한 비율을 구한다. 정밀도가 선택한 컨텐츠의 정확성을 보는 기준이라면 재현율은 IR이 관련있는 컨텐츠들을 얼마나 놓치지 않고 가져올 수 있는지를 가늠할 수 있는 기준이 된다.

이해를 돕기 위해 하루에 100개씩 부품을 만드는 공장이 있다고 가정하자.
이 중 90개는 정상 부품과 10개는 불량품이고, 이러한 불량품을 걸러내기 위해 완성된 부품을 카메라로 찍어 불량품을 걸러내는 IR이 있다.
금일 생산된 부품에 대하여 IR을 돌려본 결과, IR은 7개의 불량품을 선택하였고 이 중 6개는 실제 불량품이었고, 1개는 정상부품이었다.
이 IR의 정확도, 정밀도, 재현율을 각각 어떻게 될까?

우선 tp, tn, fp, fn을 구하면 다음과 같다.

![tp_tn_fp_fn]({{site_url}}/assets/images/2020-03-16-01-03.png)

1. tp = 불량품으로 선택된 부품 중 불량품은 6개
1. tn = 불량품으로 선택되지 않은 부품 중 정상부품은 89개 
1. fp = 불량품으로 선택된 부품 중 정상부품은 1개 
1. fn = 불량품으로 선택되지 않은 부품 중 불량품은 4개 

위 값을 토대로 정확도, 정밀도, 재현율을 계산하면 다음과 같다.

1. 정확도(accuracy) = tp + tn / tp + tn + fp + fn  = (6 + 89) / 100 = 0.95
1. 정밀도(precision) = tp / tp + fp = 6 / (6 + 1) = 0.85
1. 재현율(recall) = tp / tp + fn = 6 / (6 + 4) = 0.6

이렇게 계산된 값 중 0.95의 정확도는 전체 부품 중 95%에 해당하는 부품(정산부품이나 불량품을 가리지 않고)을 정확히 판단할 수 있다는 의미이다.
0.85의 정밀도는 IR이 불량품이라고 판단한 부품 중 85%에 해당하는 부품이 실제 불량품이라는 의미이다.
0.6이라는 재현율은 실제 불량품 중 IR이 불량품이라고 판단할 확률이 60%라는 의미를 갖는다.

위에서 언급했듯이 정확도, 정밀도, 재현율을 각기 다른 의미를 가지는 IR 평가 기준이므로 각 IR에 맞게 평가 기준을 세워야 한다.

정확도, 정밀도, 재현율은 [precision and recall] 글을 보면 잘 설명해주고 있으니 참고하자.

## 마치며

이 글에서는 IR의 평가기준이 될 수 있는 정확도, 정밀도, 재현율에 대해서 살펴보았다.

1. tp, tn, fp, fn의 개념을 익히자.
1. 정확도, 정밀도, 재현율의 개념을 이해하자.
1. 다양한 평가 기준 중 IR이 중점적으로 봐야 하는 특성에 맞는 값을 기준으로 사용할 수 있다.

다음 글에서는 정밀도, 재현율을 발전시킨 평가모델인 MRR, MAP, NDCG에 대해 알아보도록 하자.

[precision and recall]: https://en.wikipedia.org/wiki/Precision_and_recall
